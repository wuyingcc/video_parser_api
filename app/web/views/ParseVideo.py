import asyncio
import os
import time
from typing import Optional, List

import yaml
from fastapi import HTTPException
from pywebio.input import *
from pywebio.output import *
from pywebio_battery import put_video
from pydantic import BaseModel
from app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # å¯¼å…¥å“åº”æ¨¡å‹
from app.api.router import router
from app.web.views.ViewsUtils import ViewsUtils
from fastapi import APIRouter, Body, Query, Request, HTTPException  # å¯¼å…¥FastAPIç»„ä»¶
from crawlers.hybrid.hybrid_crawler import HybridCrawler

HybridCrawler = HybridCrawler()

# è¯»å–ä¸Šçº§å†ä¸Šçº§ç›®å½•çš„é…ç½®æ–‡ä»¶
config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), 'config.yaml')
with open(config_path, 'r', encoding='utf-8') as file:
    config = yaml.safe_load(file)


# æ ¡éªŒè¾“å…¥å€¼/Validate input value
def valid_check(input_data: str):
    # æ£€ç´¢å‡ºæ‰€æœ‰é“¾æ¥å¹¶è¿”å›åˆ—è¡¨/Retrieve all links and return a list
    url_list = ViewsUtils.find_url(input_data)
    # æ€»å…±æ‰¾åˆ°çš„é“¾æ¥æ•°é‡/Total number of links found
    total_urls = len(url_list)
    if total_urls == 0:
        warn_info = ViewsUtils.t('æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„é“¾æ¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥çš„å†…å®¹æ˜¯å¦æ­£ç¡®ã€‚',
                                 'No valid link detected, please check if the input content is correct.')
        return warn_info
    else:
        # æœ€å¤§æ¥å—æäº¤URLçš„æ•°é‡/Maximum number of URLs accepted
        max_urls = config['Web']['Max_Take_URLs']
        if total_urls > int(max_urls):
            warn_info = ViewsUtils.t(f'è¾“å…¥çš„é“¾æ¥å¤ªå¤šå•¦ï¼Œå½“å‰åªä¼šå¤„ç†è¾“å…¥çš„å‰{max_urls}ä¸ªé“¾æ¥ï¼',
                                     f'Too many links input, only the first {max_urls} links will be processed!')
            return warn_info


# é”™è¯¯å¤„ç†/Error handling
def error_do(reason: str, value: str) -> None:
    # è¾“å‡ºä¸€ä¸ªæ¯«æ— ç”¨å¤„çš„ä¿¡æ¯
    put_html("<hr>")
    put_error(
        ViewsUtils.t("å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ï¼Œç¨‹åºå°†è·³è¿‡è¿™ä¸ªè¾“å…¥å€¼ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¾“å…¥å€¼ã€‚",
                     "An error occurred, the program will skip this input value and continue to process the next input value."))
    put_html(f"<h3>âš {ViewsUtils.t('è¯¦æƒ…', 'Details')}</h3>")
    put_table([
        [
            ViewsUtils.t('åŸå› ', 'reason'),
            ViewsUtils.t('è¾“å…¥å€¼', 'input value')
        ],
        [
            reason,
            value
        ]
    ])
    put_markdown(ViewsUtils.t('> å¯èƒ½çš„åŸå› :', '> Possible reasons:'))
    put_markdown(ViewsUtils.t("- è§†é¢‘å·²è¢«åˆ é™¤æˆ–è€…é“¾æ¥ä¸æ­£ç¡®ã€‚",
                              "- The video has been deleted or the link is incorrect."))
    put_markdown(ViewsUtils.t("- æ¥å£é£æ§ï¼Œè¯·æ±‚è¿‡äºé¢‘ç¹ã€‚",
                              "- Interface risk control, request too frequent.")),
    put_markdown(ViewsUtils.t("- æ²¡æœ‰ä½¿ç”¨æœ‰æ•ˆçš„Cookieï¼Œå¦‚æœä½ éƒ¨ç½²åæ²¡æœ‰æ›¿æ¢ç›¸åº”çš„Cookieï¼Œå¯èƒ½ä¼šå¯¼è‡´è§£æå¤±è´¥ã€‚",
                              "- No valid Cookie is used. If you do not replace the corresponding Cookie after deployment, it may cause parsing failure."))
    put_markdown(ViewsUtils.t("> å¯»æ±‚å¸®åŠ©:", "> Seek help:"))
    put_markdown(ViewsUtils.t(
        "- ä½ å¯ä»¥å°è¯•å†æ¬¡è§£æï¼Œæˆ–è€…å°è¯•è‡ªè¡Œéƒ¨ç½²é¡¹ç›®ï¼Œç„¶åæ›¿æ¢`./app/crawlers/å¹³å°æ–‡ä»¶å¤¹/config.yaml`ä¸­çš„`cookie`å€¼ã€‚",
        "- You can try to parse again, or try to deploy the project by yourself, and then replace the `cookie` value in `./app/crawlers/platform folder/config.yaml`."))

    put_markdown(
        "- GitHub Issue: [Evil0ctal/Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/issues)")
    put_html("<hr>")


def parse_video():
    placeholder = ViewsUtils.t(
        "æ‰¹é‡è§£æè¯·ç›´æ¥ç²˜è´´å¤šä¸ªå£ä»¤æˆ–é“¾æ¥ï¼Œæ— éœ€ä½¿ç”¨ç¬¦å·åˆ†å¼€ï¼Œæ”¯æŒæŠ–éŸ³å’ŒTikToké“¾æ¥æ··åˆï¼Œæš‚æ—¶ä¸æ”¯æŒä½œè€…ä¸»é¡µé“¾æ¥æ‰¹é‡è§£æã€‚",
        "Batch parsing, please paste multiple passwords or links directly, no need to use symbols to separate, support for mixing Douyin and TikTok links, temporarily not support for author home page link batch parsing.")
    input_data = textarea(
        ViewsUtils.t('è¯·å°†æŠ–éŸ³æˆ–TikTokçš„åˆ†äº«å£ä»¤æˆ–ç½‘å€ç²˜è´´äºæ­¤',
                     "Please paste the share code or URL of [Douyin|TikTok] here"),
        type=TEXT,
        validate=valid_check,
        required=True,
        placeholder=placeholder,
        position=0)
    url_lists = ViewsUtils.find_url(input_data)
    # è§£æå¼€å§‹æ—¶é—´
    start = time.time()
    # æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
    success_count = 0
    failed_count = 0
    # é“¾æ¥æ€»æ•°
    url_count = len(url_lists)
    # è§£ææˆåŠŸçš„url
    success_list = []
    # è§£æå¤±è´¥çš„url
    failed_list = []
    # è¾“å‡ºä¸€ä¸ªæç¤ºæ¡
    with use_scope('loading_text'):
        # è¾“å‡ºä¸€ä¸ªåˆ†è¡Œç¬¦
        put_row([put_html('<br>')])
        put_warning(ViewsUtils.t('Serveré…±æ­£æ”¶åˆ°ä½ è¾“å…¥çš„é“¾æ¥å•¦ï¼(â—â€¢á´—â€¢â—)\næ­£åœ¨åŠªåŠ›å¤„ç†ä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»...',
                                 'ServerChan is receiving your input link! (â—â€¢á´—â€¢â—)\nEfforts are being made, please wait a moment...'))
    # ç»“æœé¡µæ ‡é¢˜
    put_scope('result_title')
    # éå†é“¾æ¥åˆ—è¡¨
    for url in url_lists:
        # é“¾æ¥ç¼–å·
        url_index = url_lists.index(url) + 1
        # è§£æ
        try:
            data = asyncio.run(HybridCrawler.hybrid_parsing_single_video(url, minimal=True))
        except Exception as e:
            error_msg = str(e)
            with use_scope(str(url_index)):
                error_do(reason=error_msg, value=url)
            failed_count += 1
            failed_list.append(url)
            continue

        # åˆ›å»ºä¸€ä¸ªè§†é¢‘/å›¾é›†çš„å…¬æœ‰å˜é‡
        url_type = ViewsUtils.t('è§†é¢‘', 'Video') if data.get('type') == 'video' else ViewsUtils.t('å›¾ç‰‡', 'Image')
        platform = data.get('platform')
        table_list = [
            [ViewsUtils.t('ç±»å‹', 'type'), ViewsUtils.t('å†…å®¹', 'content')],
            [ViewsUtils.t('è§£æç±»å‹', 'Type'), url_type],
            [ViewsUtils.t('å¹³å°', 'Platform'), platform],
            [f'{url_type} ID', data.get('aweme_id')],
            [ViewsUtils.t(f'{url_type}æè¿°', 'Description'), data.get('desc')],
            [ViewsUtils.t('ä½œè€…æ˜µç§°', 'Author nickname'), data.get('author').get('nickname')],
            [ViewsUtils.t('ä½œè€…ID', 'Author ID'), data.get('author').get('unique_id')],
            [ViewsUtils.t('APIé“¾æ¥', 'API URL'),
             put_link(
                 ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                 f"/api/hybrid/video_data?url={url}&minimal=false",
                 new_window=True)],
            [ViewsUtils.t('APIé“¾æ¥-ç²¾ç®€', 'API URL-Minimal'),
             put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                      f"/api/hybrid/video_data?url={url}&minimal=true",
                      new_window=True)]

        ]
        # å¦‚æœæ˜¯è§†é¢‘/If it's video
        if url_type == ViewsUtils.t('è§†é¢‘', 'Video'):
            wm_video_url_HQ = data.get('video_data', {}).get('wm_video_url_HQ')
            nwm_video_url_HQ = data.get('video_data', {}).get('nwm_video_url_HQ')

            # æ·»åŠ è§†é¢‘ä¿¡æ¯
            if wm_video_url_HQ:  # ç¡®ä¿æ°´å°è§†é¢‘é“¾æ¥ä¸ä¸º None
                table_list.insert(4, [ViewsUtils.t('è§†é¢‘é“¾æ¥-æ°´å°', 'Video URL-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                                           data.get('video_data').get('wm_video_url_HQ'), new_window=True)])
            if nwm_video_url_HQ:  # ç¡®ä¿æ— æ°´å°è§†é¢‘é“¾æ¥ä¸ä¸º None
                table_list.insert(5, [ViewsUtils.t('è§†é¢‘é“¾æ¥-æ— æ°´å°', 'Video URL-No Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»æŸ¥çœ‹', 'Click to view'),
                                           data.get('video_data').get('nwm_video_url_HQ'), new_window=True)])

            table_list.insert(6, [ViewsUtils.t('è§†é¢‘ä¸‹è½½-æ°´å°', 'Video Download-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=true",
                                           new_window=True)])
            table_list.insert(7, [ViewsUtils.t('è§†é¢‘ä¸‹è½½-æ— æ°´å°', 'Video Download-No-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=false",
                                           new_window=True)])
            # æ·»åŠ è§†é¢‘ä¿¡æ¯
            table_list.insert(0, [
                put_video(data.get('video_data').get('nwm_video_url_HQ'), poster=None, loop=True, width='50%')])
        # å¦‚æœæ˜¯å›¾ç‰‡/If it's image
        elif url_type == ViewsUtils.t('å›¾ç‰‡', 'Image'):
            # æ·»åŠ å›¾ç‰‡ä¸‹è½½é“¾æ¥
            table_list.insert(4, [ViewsUtils.t('å›¾ç‰‡æ‰“åŒ…ä¸‹è½½-æ°´å°', 'Download images ZIP-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=true",
                                           new_window=True)])
            table_list.insert(5, [ViewsUtils.t('å›¾ç‰‡æ‰“åŒ…ä¸‹è½½-æ— æ°´å°', 'Download images ZIP-No-Watermark'),
                                  put_link(ViewsUtils.t('ç‚¹å‡»ä¸‹è½½', 'Click to download'),
                                           f"/api/download?url={url}&prefix=true&with_watermark=false",
                                           new_window=True)])
            # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
            no_watermark_image_list = data.get('image_data').get('no_watermark_image_list')
            for image in no_watermark_image_list:
                table_list.append(
                    [ViewsUtils.t('å›¾ç‰‡é¢„è§ˆ(å¦‚æ ¼å¼å¯æ˜¾ç¤º): ', 'Image preview (if the format can be displayed):'),
                     put_image(image, width='50%')])
                table_list.append([ViewsUtils.t('å›¾ç‰‡ç›´é“¾: ', 'Image URL:'),
                                   put_link(ViewsUtils.t('â¬†ï¸ç‚¹å‡»æ‰“å¼€å›¾ç‰‡â¬†ï¸', 'â¬†ï¸Click to open imageâ¬†ï¸'), image,
                                            new_window=True)])
        # å‘ç½‘é¡µè¾“å‡ºè¡¨æ ¼/Put table on web page
        with use_scope(str(url_index)):
            # æ˜¾ç¤ºè¿›åº¦
            put_info(
                ViewsUtils.t(f'æ­£åœ¨è§£æç¬¬{url_index}/{url_count}ä¸ªé“¾æ¥: ',
                             f'Parsing the {url_index}/{url_count}th link: '),
                put_link(url, url, new_window=True), closable=True)
            put_table(table_list)
            put_html('<hr>')
        scroll_to(str(url_index))
        success_count += 1
        success_list.append(url)
        # print(success_count: {success_count}, success_list: {success_list}')
    # å…¨éƒ¨è§£æå®Œæˆè·³å‡ºforå¾ªç¯/All parsing completed, break out of for loop
    with use_scope('result_title'):
        put_row([put_html('<br>')])
        put_markdown(ViewsUtils.t('## ğŸ“è§£æç»“æœ:', '## ğŸ“Parsing results:'))
        put_row([put_html('<br>')])
    with use_scope('result'):
        # æ¸…é™¤è¿›åº¦æ¡
        clear('loading_text')
        # æ»šåŠ¨è‡³result
        scroll_to('result')
        # forå¾ªç¯ç»“æŸï¼Œå‘ç½‘é¡µè¾“å‡ºæˆåŠŸæé†’
        put_success(ViewsUtils.t('è§£æå®Œæˆå•¦ â™ª(ï½¥Ï‰ï½¥)ï¾‰\nè¯·æŸ¥çœ‹ä»¥ä¸‹ç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœè§‰å¾—æœ‰ç”¨çš„è¯è¯·åœ¨GitHubä¸Šå¸®æˆ‘ç‚¹ä¸€ä¸ªStarå§ï¼',
                                 'Parsing completed â™ª(ï½¥Ï‰ï½¥)ï¾‰\nPlease check the following statistics, and if you think it\'s useful, please help me click a Star on GitHub!'))
        # å°†æˆåŠŸï¼Œå¤±è´¥ä»¥åŠæ€»æ•°é‡æ˜¾ç¤ºå‡ºæ¥å¹¶ä¸”æ˜¾ç¤ºä¸ºä»£ç æ–¹ä¾¿å¤åˆ¶
        put_markdown(
            f'**{ViewsUtils.t("æˆåŠŸ", "Success")}:** {success_count} **{ViewsUtils.t("å¤±è´¥", "Failed")}:** {failed_count} **{ViewsUtils.t("æ€»æ•°é‡", "Total")}:** {success_count + failed_count}')
        # æˆåŠŸåˆ—è¡¨
        if success_count != url_count:
            put_markdown(f'**{ViewsUtils.t("æˆåŠŸåˆ—è¡¨", "Success list")}:**')
            put_code('\n'.join(success_list))
        # å¤±è´¥åˆ—è¡¨
        if failed_count > 0:
            put_markdown(f'**{ViewsUtils.t("å¤±è´¥åˆ—è¡¨", "Failed list")}:**')
            put_code('\n'.join(failed_list))
        # å°†url_listsæ˜¾ç¤ºä¸ºä»£ç æ–¹ä¾¿å¤åˆ¶
        put_markdown(ViewsUtils.t('**ä»¥ä¸‹æ˜¯æ‚¨è¾“å…¥çš„æ‰€æœ‰é“¾æ¥ï¼š**', '**The following are all the links you entered:**'))
        put_code('\n'.join(url_lists))
        # è§£æç»“æŸæ—¶é—´
        end = time.time()
        # è®¡ç®—è€—æ—¶,ä¿ç•™ä¸¤ä½å°æ•°
        time_consuming = round(end - start, 2)
        # æ˜¾ç¤ºè€—æ—¶
        put_markdown(f"**{ViewsUtils.t('è€—æ—¶', 'Time consuming')}:** {time_consuming}s")
        # æ”¾ç½®ä¸€ä¸ªæŒ‰é’®ï¼Œç‚¹å‡»åè·³è½¬åˆ°é¡¶éƒ¨
        put_button(ViewsUtils.t('å›åˆ°é¡¶éƒ¨', 'Back to top'), onclick=lambda: scroll_to('1'), color='success',
                   outline=True)
        # è¿”å›ä¸»é¡µé“¾æ¥
        put_link(ViewsUtils.t('å†æ¥ä¸€æ³¢ (ã¤Â´Ï‰`)ã¤', 'Another wave (ã¤Â´Ï‰`)ã¤'), '/')




# å®šä¹‰è§†é¢‘è§£æç»“æœçš„æ•°æ®æ¨¡å‹
class VideoDataModel(BaseModel):
    url: str
    type: str
    platform: str
    aweme_id: str
    desc: str
    author_nickname: str
    author_id: str
    wm_video_url_HQ: str
    nwm_video_url_HQ: str

# å®šä¹‰å›¾ç‰‡è§£æç»“æœçš„æ•°æ®æ¨¡å‹
class ImageDataModel(BaseModel):
    url: str
    type: str
    platform: str
    aweme_id: str
    desc: str
    author_nickname: str
    author_id: str
    no_watermark_image_list: List[str]


# å®šä¹‰è§£æè§†é¢‘çš„ API
@router.post("/parse_video2", response_model=ResponseModel, summary="è§£æè§†é¢‘/parse_video2")
async def parse_video2(request: Request,input_data: str = Query(example="7.97 N@w.SY 08/17 Ljc:/ â€œæˆ‘ä»¬ä¸ä¼šå†è§é¢è¿™å°±æ˜¯åˆ†åˆ«çš„æ„ä¹‰â€# è¿ˆå·´èµ«s680 # è¿ˆå·´èµ« # å°é—­è·¯æ®µæ‹æ‘„  https://v.douyin.com/iyCoJoHh/ å¤åˆ¶æ­¤é“¾æ¥ï¼Œæ‰“å¼€DouéŸ³æœç´¢ï¼Œç›´æ¥è§‚çœ‹è§†é¢‘ï¼", description="è§†é¢‘é“¾æ¥/Video URL")):

    url_lists = valid_check2(input_data)
    result = {
        "success_count": 0,
        "failed_count": 0,
        "success_list": [],
        "failed_list": []
    }
    # è§£ææ¯ä¸ªè§†é¢‘é“¾æ¥
    for url in url_lists:
        try:
            # è§£æè§†é¢‘æ•°æ®
            data = await HybridCrawler.hybrid_parsing_single_video(url, minimal=True)
            url_type = 'Video' if data.get('type') == 'video' else 'Image'
            platform = data.get('platform')

            if url_type == 'Video':
                video_info = VideoDataModel(
                    url=url,
                    type=url_type,
                    platform=platform,
                    aweme_id=data.get('aweme_id'),
                    desc=data.get('desc'),
                    author_nickname=data.get('author', {}).get('nickname', ''),
                    author_id=data.get('author', {}).get('unique_id', ''),
                    wm_video_url_HQ=data.get('video_data', {}).get('wm_video_url_HQ', None),
                    nwm_video_url_HQ=data.get('video_data', {}).get('nwm_video_url_HQ', None)
                )
                # æˆåŠŸçš„ç»“æœ
                result["success_count"] += 1
                result["success_list"].append(video_info.dict())
            elif url_type == 'Image':
                image_info = ImageDataModel(
                    url=url,
                    type=url_type,
                    platform=platform,
                    aweme_id=data.get('aweme_id'),
                    desc=data.get('desc'),
                    author_nickname=data.get('author', {}).get('nickname', ''),
                    author_id=data.get('author', {}).get('unique_id', ''),
                    no_watermark_image_list=data.get('image_data', {}).get('no_watermark_image_list', [])
                )
                # æˆåŠŸçš„ç»“æœ
                result["success_count"] += 1
                result["success_list"].append(image_info.dict())


        except Exception as e:
            error_msg = str(e)
            result["failed_count"] += 1
            result["failed_list"].append(url)
            result["error_details"] = error_do2(reason=error_msg, value=url)
            continue
    return ResponseModel(code=200,
                     router=request.url.path,
                     data=result)



# æ ¡éªŒè¾“å…¥çš„é“¾æ¥æ ¼å¼
def valid_check2(input_data: str):
    url_list = ViewsUtils.find_url(input_data)
    total_urls = len(url_list)
    if total_urls == 0:
        raise HTTPException(status_code=400, detail="No valid link detected, please check the input.")
    return url_list

# å¼‚å¸¸å¤„ç†
def error_do2(reason: str, value: str) -> dict:
    return {
        "error": True,
        "reason": reason,
        "value": value,
        "possible_causes": [
            "Video has been deleted or the link is incorrect.",
            "Frequent requests may trigger interface protection.",
            "No valid cookie used, check if the correct cookie is set in the config.yaml file."
        ]
    }

